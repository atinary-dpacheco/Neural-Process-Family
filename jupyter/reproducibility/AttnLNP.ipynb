{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attentive Latent Neural Process (AttnLNP)\n",
    "\n",
    "```{figure} ../images/computational_graph_AttnLNPs.png\n",
    "---\n",
    "width: 25em\n",
    "name: computational_graph_AttnLNPs\n",
    "alt: Computational graph of AttnLNP\n",
    "---\n",
    "Computational graph for Attentive Latent Neural Processes.\n",
    "```\n",
    "\n",
    "In this notebook we will show how to train a AttnLNP on samples from GPs and images using our framework, as well as how to make nice visualizations of sampled from AttnLNPs.\n",
    "We will follow quite closely the previous {doc}`LNP notebook <LNP>` and {doc}`AttnCNP notebook <AttnCNP>`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "\n",
    "N_THREADS = 8\n",
    "IS_FORCE_CPU = False  # Nota Bene : notebooks don't deallocate GPU memory\n",
    "\n",
    "if IS_FORCE_CPU:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "# reset GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.ipc_collect()\n",
    "torch.set_num_threads(N_THREADS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Let's load all the data. For more details about the data and some samples, see the {doc}`data <Datasets>` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import get_all_gp_datasets, get_img_datasets\n",
    "\n",
    "# DATASETS\n",
    "# gp\n",
    "gp_datasets, gp_test_datasets, gp_valid_datasets = get_all_gp_datasets()\n",
    "# image\n",
    "# img_datasets, img_test_datasets = get_img_datasets([\"celeba32\", \"mnist\", \"zsmms\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the context target splitters, which given a data point will return the context set and target set by selecting randomly selecting some points and preprocessing them so that the features are in $[-1,1]$. \n",
    "We use the same as in {doc}`CNP notebook <CNP>`, namely all target points and uniformly sampling in $[0,50]$ and $[0,n\\_pixels * 0.3]$ for 1D and 2D respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from npf.utils.datasplit import (\n",
    "    CntxtTrgtGetter,\n",
    "    GetRandomIndcs,\n",
    "    GridCntxtTrgtGetter,\n",
    "    RandomMasker,\n",
    "    get_all_indcs,\n",
    "    no_masker,\n",
    ")\n",
    "from utils.data import cntxt_trgt_collate, get_test_upscale_factor\n",
    "\n",
    "# CONTEXT TARGET SPLIT\n",
    "get_cntxt_trgt_1d = cntxt_trgt_collate(\n",
    "    CntxtTrgtGetter(\n",
    "        contexts_getter=GetRandomIndcs(a=0.0, b=50), targets_getter=get_all_indcs,\n",
    "    )\n",
    ")\n",
    "get_cntxt_trgt_2d = cntxt_trgt_collate(\n",
    "    GridCntxtTrgtGetter(\n",
    "        context_masker=RandomMasker(a=0.0, b=0.3), target_masker=no_masker,\n",
    "    )\n",
    ")\n",
    "\n",
    "# for ZSMMS you need the pixels to not be in [-1,1] but [-1.75,1.75] (i.e 56 / 32) because you are extrapolating\n",
    "get_cntxt_trgt_2d_extrap = cntxt_trgt_collate(\n",
    "    GridCntxtTrgtGetter(\n",
    "        context_masker=RandomMasker(a=0, b=0.3),\n",
    "        target_masker=no_masker,\n",
    "        upscale_factor=get_test_upscale_factor(\"zsmms\"),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cntxt_trgt_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the models. We use the same architecture as in {doc}`AttnCNP notebook <AttnCNP>`. The only differences are that we replace `AttnCNP` with `AttnLNP`, as a result we will use two paths ({numref}`computational_graph_AttnCNPs`): \n",
    "* **Deterministic Path**: this is the same  as in {doc}`AttnCNP notebook <AttnCNP>`.\n",
    "* **Latent Path**: this is the same as in {doc}`LNP notebook <LNP>`.\n",
    "\n",
    "As in {doc}`LNP notebook <LNP>` we will train the model using NPVI and thus set `is_q_zCct` to infer the latent variable using BOTH the context and target set (posterior sampling). This also means that when evaluating we will evaluate the log likelihood using posterior sampling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from npf import AttnLNP, SemoptNP\n",
    "from npf.architectures import MLP, merge_flat_input\n",
    "from utils.helpers import count_parameters\n",
    "\n",
    "R_DIM = 128\n",
    "batch_size = 64\n",
    "KWARGS = dict(\n",
    "    is_q_zCct=True,  # will use NPVI => posterior sampling\n",
    "    n_z_samples_train=1,\n",
    "    n_z_samples_test=8,  # small number of sampled because Attn is memory intensive\n",
    "    r_dim=R_DIM,\n",
    "    attention=\"transformer\",  # multi headed attention with normalization and skip connections\n",
    ")\n",
    "\n",
    "# 1D case\n",
    "model_1d = partial(\n",
    "    AttnLNP,\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    XYEncoder=merge_flat_input(  # MLP takes single input but we give x and y so merge them\n",
    "        partial(MLP, n_hidden_layers=2, hidden_size=R_DIM), is_sum_merge=True,\n",
    "    ),\n",
    "    is_self_attn=False,\n",
    "    **KWARGS,\n",
    ")\n",
    "semopt_np = partial(SemoptNP,x_dim=1,y_dim=1,batch_size=batch_size,**KWARGS)\n",
    "\n",
    "# image (2D) case\n",
    "# model_2d = partial(\n",
    "#     AttnLNP, x_dim=2, is_self_attn=True, **KWARGS\n",
    "# )  # don't add y_dim yet because depends on data\n",
    "\n",
    "n_params_1d = count_parameters(model_1d())\n",
    "n_params_semopt = count_parameters(semopt_np())\n",
    "# n_params_2d = count_parameters(model_2d(y_dim=3))\n",
    "print(f\"Number Parameters (1D): {n_params_1d:,d}\")\n",
    "print(f\"Number Parameters (semopt): {n_params_semopt:,d}\")\n",
    "# print(f\"Number Parameters (2D): {n_params_2d:,d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are more parameters than in `CNP notebook <CNP>` because of the latent path. \n",
    "For more details about all the possible parameters, refer to the docstrings of `AttnLNP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [],
   "source": [
    "# AttnLNP Docstring\n",
    "\n",
    "print(SemoptNP.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The main function for training is `train_models` which trains a dictionary of models on a dictionary of datasets and returns all the trained models.\n",
    "See its docstring for possible parameters. As in {doc}`LNP notebook <LNP>` we train the latent variable model with NPVI`ELBOLossLNPF`. \n",
    "\n",
    "\n",
    "\n",
    "Computational Notes :\n",
    "- the following will either train all the models (`is_retrain=True`) or load the pretrained models (`is_retrain=False`)\n",
    "- the code will use a (single) GPU if available\n",
    "- decrease the batch size if you don't have enough memory\n",
    "- 30 epochs should give you descent results for the GP datasets (instead of 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skorch\n",
    "from npf import ELBOLossLNPF\n",
    "from utils.ntbks_helpers import add_y_dim\n",
    "from utils.train import train_models\n",
    "\n",
    "KWARGS = dict(\n",
    "    is_retrain=True,  # whether to load precomputed model or retrain\n",
    "    criterion=ELBOLossLNPF,  # NPVI\n",
    "    chckpnt_dirname=\"results/pretrained_9/\",\n",
    "    # chckpnt_dirname=\"results/pretrained_4/\",\n",
    "    device=None,  # use GPU if available\n",
    "    batch_size=batch_size,\n",
    "    lr=1e-3,\n",
    "    decay_lr=10,  # decrease learning rate by 10 during training\n",
    "    seed=123,\n",
    ")\n",
    "\n",
    "\n",
    "# 1D\n",
    "trainers_1d = train_models(\n",
    "    gp_datasets,\n",
    "    {\n",
    "        # \"AttnLNP\": model_1d,\n",
    "        \"SeMOpt\": semopt_np,\n",
    "    },\n",
    "    test_datasets=gp_test_datasets,\n",
    "    iterator_train__collate_fn=get_cntxt_trgt_1d,\n",
    "    iterator_valid__collate_fn=get_cntxt_trgt_1d,\n",
    "    max_epochs=20,\n",
    "    **KWARGS\n",
    ")\n",
    "\n",
    "\n",
    "# 2D\n",
    "# trainers_2d = train_models(\n",
    "#     img_datasets,\n",
    "#     add_y_dim({\"AttnLNP\": model_2d}, img_datasets),  # y_dim (channels) depend on data\n",
    "#     test_datasets=img_test_datasets,\n",
    "#     train_split=skorch.dataset.CVSplit(0.1),  # use 10% of training for valdiation\n",
    "#     iterator_train__collate_fn=get_cntxt_trgt_2d,\n",
    "#     iterator_valid__collate_fn=get_cntxt_trgt_2d,\n",
    "#     datasets_kwargs=dict(\n",
    "#         zsmms=dict(\n",
    "#             iterator_valid__collate_fn=get_cntxt_trgt_2d_extrap,\n",
    "#         )\n",
    "#     ),  # for zsmm use extrapolation\n",
    "#     max_epochs=50,\n",
    "#     **KWARGS\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots\n",
    "\n",
    "Let's visualize how well the model performs in different settings.\n",
    "\n",
    "#### GPs Dataset\n",
    "\n",
    "Let's define a plotting function that we will use in this section. We'll reuse the same plotting procedure as in {doc}`LNP notebook <LNP>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.ntbks_helpers import PRETTY_RENAMER, plot_multi_posterior_samples_1d\n",
    "from utils.visualize import giffify\n",
    "\n",
    "\n",
    "def multi_posterior_gp_gif(filename, trainers, datasets, seed=123, **kwargs):\n",
    "    giffify(\n",
    "        save_filename=f\"jupyter/gifs/new5/{filename}.gif\",\n",
    "        gen_single_fig=plot_multi_posterior_samples_1d,  # core plotting\n",
    "        sweep_parameter=\"n_cntxt\",  # param over which to sweep\n",
    "        sweep_values=[0, 2, 5, 7, 10, 15, 20, 30, 50, 100],\n",
    "        fps=1.,  # gif speed\n",
    "        # PLOTTING KWARGS\n",
    "        trainers=trainers,\n",
    "        datasets=datasets,\n",
    "        is_plot_generator=True,  # plot underlying GP\n",
    "        is_plot_real=False,  # don't plot sampled / underlying function\n",
    "        is_plot_std=True,  # plot the predictive std\n",
    "        is_fill_generator_std=False,  # do not fill predictive of GP\n",
    "        pretty_renamer=PRETTY_RENAMER,  # pretiffy names of modulte + data\n",
    "        # Fix formatting for coherent GIF\n",
    "        plot_config_kwargs=dict(\n",
    "            set_kwargs=dict(ylim=[-3, 3]), rc={\"legend.loc\": \"upper right\"}\n",
    "        ),\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us visualize samples from the LNP when it is trained on samples from a single GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_single_gp(d):\n",
    "    \"\"\"Select only data form single GP.\"\"\"\n",
    "    return {k: v for k, v in d.items() if (\"All\" not in k) and (\"Variable\" not in k)}\n",
    "\n",
    "\n",
    "multi_posterior_gp_gif(\n",
    "    \"SeMOpt updated sampling 20 epochs\",\n",
    "    # \"SeMOpt_single_gp_30_epochs\",\n",
    "    # \"AttnLNP_single_gp_20_epochs_bs_64\",\n",
    "    # \"AttnLNP_single_gp_pretrained\",\n",
    "    trainers=filter_single_gp(trainers_1d),\n",
    "    datasets=filter_single_gp(gp_test_datasets),\n",
    "    n_samples=20,  # 20 samples from the latent\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../gifs/AttnLNP_single_gp.gif\n",
    "---\n",
    "width: 35em\n",
    "name: AttnLNP_single_gp\n",
    "alt: AttnLNP on single GP\n",
    "---\n",
    "\n",
    "Posterior predictive of AttnLNPs conditioned on 20 different sampled latents (Blue line with shaded area for $\\mu \\pm \\sigma | z$) and the oracle GP (Green line with dashes for $\\mu \\pm \\sigma$) when conditioned on contexts points (Black) from an underlying function sampled from a GP. Each row corresponds to a different kernel and AttnLNP trained on samples for the corresponding GP. \n",
    "```\n",
    "\n",
    "From {numref}`AttnLNP_single_gp` we see that although AttnLNP do not underfit like LNPs ({numref}`LNP_single_gp`). The samples: (i) are not very smooth (the \"kinks\" seed in {numref}`AttnCNP_single_gp` are even more obvious when sampling); (ii) lack diversity and seem to be shifted versions of each other. In addition AttnLNP will not be able to extrapolate for the same reasons as AttnCNP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ###### ADDITIONAL 1D PLOTS ######\n",
    "\n",
    "# ### Extrap ###\n",
    "# multi_posterior_gp_gif(\n",
    "#     \"AttnLNP_single_gp_extrap\",\n",
    "#     trainers=filter_single_gp(trainers_1d),\n",
    "#     datasets=filter_single_gp(gp_test_datasets),\n",
    "#     left_extrap=-2,  # shift signal 2 to the right for extrapolation\n",
    "#     right_extrap=2,  # shift signal 2 to the right for extrapolation\n",
    "#     n_samples=20,\n",
    "# )\n",
    "\n",
    "# ### Varying hyperparam ###\n",
    "# def filter_hyp_gp(d):\n",
    "#     return {k: v for k, v in d.items() if (\"Variable\" in k)}\n",
    "\n",
    "\n",
    "# multi_posterior_gp_gif(\n",
    "#     \"AttnLNP_vary_gp\",\n",
    "#     trainers=filter_hyp_gp(trainers_1d),\n",
    "#     datasets=filter_hyp_gp(gp_test_datasets),\n",
    "#     n_samples=20,\n",
    "#     model_labels=dict(main=\"Model\", generator=\"Fitted GP\")\n",
    "# )\n",
    "\n",
    "# ### All kernels ###\n",
    "# # data with varying kernels simply merged single kernels\n",
    "# single_gp_datasets = filter_single_gp(gp_test_datasets)\n",
    "\n",
    "# # use same trainer for all, but have to change their name to be the same as datasets\n",
    "# base_trainer_name = \"All_Kernels/AttnLNP/run_0\"\n",
    "# trainer = trainers_1d[base_trainer_name]\n",
    "# replicated_trainers = {}\n",
    "# for name in single_gp_datasets.keys():\n",
    "#     replicated_trainers[base_trainer_name.replace(\"All_Kernels\", name)] = trainer\n",
    "\n",
    "# multi_posterior_gp_gif(\n",
    "#     \"AttnLNP_kernel_gp\",\n",
    "#     trainers=replicated_trainers,\n",
    "#     datasets=single_gp_datasets,\n",
    "#     n_samples=20,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Dataset\n",
    "\n",
    "Let us now look at images. We again will use the same plotting procedure as in {doc}`LNP notebook <LNP>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils.ntbks_helpers import plot_multi_posterior_samples_imgs\n",
    "# from utils.visualize import giffify\n",
    "\n",
    "\n",
    "# def multi_posterior_imgs_gif(filename, trainers, datasets, seed=123, **kwargs):\n",
    "#     giffify(\n",
    "#         save_filename=f\"jupyter/gifs/{filename}.gif\",\n",
    "#         gen_single_fig=plot_multi_posterior_samples_imgs,  # core plotting\n",
    "#         sweep_parameter=\"n_cntxt\",  # param over which to sweep\n",
    "#         sweep_values=[\n",
    "#             0,  \n",
    "#             0.005,\n",
    "#             0.01,\n",
    "#             0.02,\n",
    "#             0.05,\n",
    "#             0.1,\n",
    "#             0.15,\n",
    "#             0.2,\n",
    "#             0.3,\n",
    "#             0.5,\n",
    "#             \"hhalf\",  # horizontal half of the image\n",
    "#             \"vhalf\",  # vertival half of the image\n",
    "#         ],\n",
    "#         fps=1.,  # gif speed\n",
    "#         # PLOTTING KWARGS\n",
    "#         trainers=trainers,\n",
    "#         datasets=datasets,\n",
    "#         n_plots=3,  # images per datasets\n",
    "#         is_plot_std=True,  # plot the predictive std\n",
    "#         pretty_renamer=PRETTY_RENAMER,  # pretiffy names of modulte + data\n",
    "#         plot_config_kwargs={\"font_scale\":0.7},\n",
    "#         # Fix formatting for coherent GIF\n",
    "#         seed=seed,\n",
    "#         **kwargs,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi_posterior_imgs_gif(\n",
    "#     \"AttnLNP_img\", trainers=trainers_2d, datasets=img_test_datasets, n_samples=3,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../gifs/AttnLNP_img.gif\n",
    "---\n",
    "width: 45em\n",
    "name: AttnLNP_img\n",
    "alt: AttnLNP on CelebA, MNIST, ZSMM\n",
    "---\n",
    "\n",
    "3 samples (means conditioned on different samples from the latent) of the posterior predictive of a AttnLNP_img for CelebA $32\\times32$, MNIST, and ZSMM for different context sets. The last row shows the standard deviation of the posterior predictive corresponding to the last sample.\n",
    "```\n",
    "\n",
    "From {numref}`AttnLNP_img` shows descent sampling and good performances when the model does not require generalization (CelebA $32\\times32$, MNIST) but breaks for ZSMM.\n",
    "\n",
    "Here are more samples, corresponding to specific percentiles of the test log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# from utils.ntbks_helpers import PRETTY_RENAMER\n",
    "# from utils.visualize import plot_qualitative_with_kde\n",
    "\n",
    "\n",
    "# n_trainers = len(trainers_2d)\n",
    "# for i, (k, trainer) in enumerate(trainers_2d.items()):\n",
    "#     data_name = k.split(\"/\")[0]\n",
    "#     model_name = k.split(\"/\")[1]\n",
    "#     dataset = img_test_datasets[data_name]\n",
    "\n",
    "#     plot_qualitative_with_kde(\n",
    "#         [PRETTY_RENAMER[model_name], trainer],\n",
    "#         dataset,\n",
    "#         figsize=(6, 6),\n",
    "#         percentiles=[1, 10, 20, 30, 50, 100],  # desired test percentile\n",
    "#         height_ratios=[1, 6],  # kde / image ratio\n",
    "#         is_smallest_xrange=True,  # rescale X axis based on percentile\n",
    "#         h_pad=0,  # padding\n",
    "#         title=PRETTY_RENAMER[data_name],\n",
    "#         upscale_factor=get_test_upscale_factor(data_name),\n",
    "#         n_samples=3,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
